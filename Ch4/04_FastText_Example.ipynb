{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FIToZHAhz2O"
   },
   "source": [
    "In this notebook we will demonstrate using the fastText library to perform text classificatoin on the dbpedie data which can we downloaded from [here](https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz). <br>fastText is a library for learning of word embeddings and text classification created by Facebook's AI Research (FAIR) lab. The model allows to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages(source: [wiki](https://en.wikipedia.org/wiki/FastText)).<br>\n",
    "**Note**: This notebook uses an older version of fasttext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBnT5t_LiCU2",
    "outputId": "ca0bcea9-75a7-4237-e58e-154c3d72e89f"
   },
   "outputs": [],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# !pip install pandas==1.1.5\n",
    "# !pip install wget==3.2\n",
    "# !pip install fasttext==0.9.2\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zrBi6bvbiCU4"
   },
   "outputs": [],
   "source": [
    "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# try:\n",
    "#     import google.colab\n",
    "#     !curl  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/ch4-requirements.txt | xargs -n 1 -L 1 pip install\n",
    "# except ModuleNotFoundError:\n",
    "#     !pip install -r \"ch4-requirements.txt\"\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YKgZXvTGb61z"
   },
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import wget\n",
    "import tarfile\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6CfW7C3L4EB",
    "outputId": "debf3639-77d2-4a2c-8aa1-3ff8438b9585"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k&confirm=t&uuid=a7accc21-6f49-4d9d-b323-f6fedaec8e8f\n",
      "To: /root/Working/Working/practical-nlp-code/Ch4/Data/dbpedia_csv.tar.gz\n",
      "100%|█████████████████████████████████████████████████████████████| 68.3M/68.3M [00:09<00:00, 7.58MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Present at location : ./Data/dbpedia_csv\n"
     ]
    }
   ],
   "source": [
    "def check_if_file_exists(filename: str, locations: list) -> str :\n",
    "    for location in locations:\n",
    "        if os.path.exists(os.path.join(location, filename)):\n",
    "            return location\n",
    "    return None\n",
    "\n",
    "def extract_tar_file(file_path: str, extraction_path: str) -> None:\n",
    "    tar = tarfile.open(file_path, \"r:gz\")\n",
    "    tar.extractall(extraction_path)\n",
    "    tar.close()\n",
    "\n",
    "try :\n",
    "    \n",
    "    from google.colab import files\n",
    "    \n",
    "    # downloading the data\n",
    "    !wget -P DATAPATH https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\n",
    "\n",
    "    # untaring the required file\n",
    "    !tar -xvf DATAPATH/dbpedia_csv.tar.gz -C DATAPATH\n",
    "\n",
    "    # sneek peek in the folder structure\n",
    "    !ls -lah DATAPATH\n",
    "    \n",
    "    # specifying the data_path\n",
    "    data_path = 'DATAPATH'\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "    data_path = './Data/'\n",
    "    compressed_file_name = 'dbpedia_csv.tar.gz'\n",
    "    extracted_file_name = 'dbpedia_csv'\n",
    "    \n",
    "    # Check if Extracted File exists\n",
    "    location_of_extracted_file = check_if_file_exists(extracted_file_name, ['./Data'])\n",
    "    \n",
    "    if location_of_extracted_file:\n",
    "        # Extracted File exists\n",
    "        path_to_model = os.path.join(location_of_extracted_file, extracted_file_name)\n",
    "        \n",
    "    else:\n",
    "        location_of_compressed_file = check_if_file_exists(compressed_file_name, ['./Data'])\n",
    "        \n",
    "        if location_of_compressed_file:\n",
    "            # Compressed File exists\n",
    "            extract_tar_file(os.path.join(location_of_compressed_file, compressed_file_name), data_path)\n",
    "            path_to_model = os.path.join(data_path, extracted_file_name)\n",
    "            \n",
    "        else:\n",
    "            # Download File\n",
    "            output_path = './Data/'\n",
    "            gdown.download(\"https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k\", output=output_path)\n",
    "\n",
    "            # Extract File\n",
    "            extract_data(output_path+compressed_file_name, output_path)\n",
    "\n",
    "            path_to_model = os.path.join(data_path, extracted_file_name)\n",
    "\n",
    "    print(f\"Data Present at location : {path_to_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMoRw3oQb62I",
    "outputId": "744d1cb7-4966-4db1-b176-c2020975ed94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:(560000, 3) Test:(70000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Loading train data\n",
    "train_file = data_path + '/dbpedia_csv/train.csv'\n",
    "df = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "# Loading test data\n",
    "test_file = data_path + '/dbpedia_csv/test.csv'\n",
    "df_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "# Data we have\n",
    "print(\"Train:{} Test:{}\".format(df.shape,df_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "gaz226vXb62W",
    "outputId": "a7e5ab41-732e-4a94-def6-5e62124d6bd5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E. D. Abbott Ltd</td>\n",
       "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                               name  \\\n",
       "0      1                   E. D. Abbott Ltd   \n",
       "1      1                     Schwan-Stabilo   \n",
       "2      1                         Q-workshop   \n",
       "3      1  Marvell Software Solutions Israel   \n",
       "4      1        Bergan Mercy Medical Center   \n",
       "\n",
       "                                         description class_name  \n",
       "0   Abbott of Farnham E D Abbott Limited was a Br...    Company  \n",
       "1   Schwan-STABILO is a German maker of pens for ...    Company  \n",
       "2   Q-workshop is a Polish company located in Poz...    Company  \n",
       "3   Marvell Software Solutions Israel known as RA...    Company  \n",
       "4   Bergan Mercy Medical Center is a hospital loc...    Company  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we have no clue about the classes lets build one\n",
    "# Mapping from class number to class name\n",
    "class_dict={\n",
    "            1:'Company',\n",
    "            2:'EducationalInstitution',\n",
    "            3:'Artist',\n",
    "            4:'Athlete',\n",
    "            5:'OfficeHolder',\n",
    "            6:'MeanOfTransportation',\n",
    "            7:'Building',\n",
    "            8:'NaturalPlace',\n",
    "            9:'Village',\n",
    "            10:'Animal',\n",
    "            11:'Plant',\n",
    "            12:'Album',\n",
    "            13:'Film',\n",
    "            14:'WrittenWork'\n",
    "        }\n",
    "\n",
    "# Mapping the classes\n",
    "df['class_name'] = df['class'].map(class_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "si7VC_Rub62a",
    "outputId": "a1f7d406-0e9c-4adf-eaee-fc09572f27bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_name\n",
       "Company                   40000\n",
       "EducationalInstitution    40000\n",
       "Artist                    40000\n",
       "Athlete                   40000\n",
       "OfficeHolder              40000\n",
       "MeanOfTransportation      40000\n",
       "Building                  40000\n",
       "NaturalPlace              40000\n",
       "Village                   40000\n",
       "Animal                    40000\n",
       "Plant                     40000\n",
       "Album                     40000\n",
       "Film                      40000\n",
       "WrittenWork               40000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Sn-3kIqMb62d"
   },
   "outputs": [],
   "source": [
    "# Lets do some cleaning of this text\n",
    "def clean_it(text,normalize=True):\n",
    "    # Replacing possible issues with data. We can add or reduce the replacemtent in this chain\n",
    "    s = str(text).replace(',',' ').replace('\"','').replace('\\'',' \\' ').replace('.',' . ').replace('(',' ( ').\\\n",
    "            replace(')',' ) ').replace('!',' ! ').replace('?',' ? ').replace(':',' ').replace(';',' ').lower()\n",
    "    \n",
    "    # normalizing / encoding the text\n",
    "    if normalize:\n",
    "        s = s.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8')\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Now lets define a small function where we can use above cleaning on datasets\n",
    "def clean_df(data, cleanit= False, shuffleit=False, encodeit=False, label_prefix='__class__'):\n",
    "    # Defining the new data\n",
    "    df = data[['name','description']].copy(deep=True)\n",
    "    df['class'] = label_prefix + data['class'].astype(str) + ' '\n",
    "    \n",
    "    # cleaning it\n",
    "    if cleanit:\n",
    "        df['name'] = df['name'].apply(lambda x: clean_it(x,encodeit))\n",
    "        df['description'] = df['description'].apply(lambda x: clean_it(x,encodeit))\n",
    "    \n",
    "    # shuffling it\n",
    "    if shuffleit:\n",
    "        df.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_DRvdFcb62m",
    "outputId": "d3fc1348-fcb2-4f50-c090-067e5ca66301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.98 s, sys: 206 ms, total: 4.19 s\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Transform the datasets using the above clean functions\n",
    "df_train_cleaned = clean_df(df, True, True)\n",
    "df_test_cleaned = clean_df(df_test, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "imMZ9-Bkb62t"
   },
   "outputs": [],
   "source": [
    "# Write files to disk as fastText classifier API reads files from disk.\n",
    "train_file = data_path + '/dbpedia_train.csv'\n",
    "df_train_cleaned.to_csv(train_file, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file = data_path + '/dbpedia_test.csv'\n",
    "df_test_cleaned.to_csv(test_file, header=None, index=False, columns=['class','name','description'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWZTSzd9b62x"
   },
   "source": [
    "Now that we have the train and test files written into disk in a format fastText wants, we are ready to use it for text classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-H1wouCb62x",
    "outputId": "3d7c130a-fd3b-472c-8585-2e965017763f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 31M words\n",
      "Number of words:  1116962\n",
      "Number of labels: 14\n",
      "Progress: 100.0% words/sec/thread: 1168450 lr:  0.000174 avg.loss:  0.003673 ETA:   0h 0m 0s 23.8% words/sec/thread: 1198254 lr:  0.762147 avg.loss:  0.009301 ETA:   0h12m38s% words/sec/thread: 1173386 lr:  0.577732 avg.loss:  0.006055 ETA:   0h 9m47s 0.419437 avg.loss:  0.004894 ETA:   0h 7m 5s 73.1% words/sec/thread: 1159819 lr:  0.268968 avg.loss:  0.004059 ETA:   0h 4m36s 0.003774 ETA:   0h 3m23s 81.4% words/sec/thread: 1157751 lr:  0.186418 avg.loss:  0.003744 ETA:   0h 3m12s 0.003730 ETA:   0h 3m 7s 82.8% words/sec/thread: 1155522 lr:  0.172032 avg.loss:  0.003694 ETA:   0h 2m57s 96.3% words/sec/thread: 1169432 lr:  0.036623 avg.loss:  0.003380 ETA:   0h 0m37s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33min 7s, sys: 19 s, total: 33min 26s\n",
      "Wall time: 17min 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread: 1168425 lr:  0.000000 avg.loss:  0.003688 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Using fastText for feature extraction and training\n",
    "from fasttext import train_supervised \n",
    "\"\"\"fastText expects and training file (csv), a model name as input arguments.\n",
    "label_prefix refers to the prefix before label string in the dataset.\n",
    "default is __label__. In our dataset, it is __class__. \n",
    "There are several other parameters which can be seen in: \n",
    "https://pypi.org/project/fasttext/\n",
    "\"\"\"\n",
    "model = train_supervised(input=train_file, label=\"__class__\", lr=1.0, epoch=75, loss='ova', wordNgrams=2, dim=200, thread=2, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAyN3ZDbQFq-",
    "outputId": "13acbc62-48d9-469c-dfb1-d3e5446b8530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Samples: 70000 Precision@1 : 90.7343 Recall@1 : 90.7343\n",
      "Test Samples: 70000 Precision@2 : 48.0407 Recall@2 : 96.0814\n",
      "Test Samples: 70000 Precision@3 : 32.3319 Recall@3 : 96.9957\n",
      "Test Samples: 70000 Precision@4 : 24.3021 Recall@4 : 97.2086\n",
      "Test Samples: 70000 Precision@5 : 19.4711 Recall@5 : 97.3557\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,6):\n",
    "    results = model.test(test_file,k=k)\n",
    "    print(f\"Test Samples: {results[0]} Precision@{k} : {results[1]*100:2.4f} Recall@{k} : {results[2]*100:2.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrxSYRs3b621"
   },
   "source": [
    "Try training a classifier on this dataset with, say, LogisticRegression to realize how fast fastText is! 90% Precision and Recall are hard numbers to beat, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "04_FastText_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
