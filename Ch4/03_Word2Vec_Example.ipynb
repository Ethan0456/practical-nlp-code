{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVtvH58nb_Hp"
      },
      "source": [
        "# Word2Vec for Text Classification\n",
        "\n",
        "In this short notebook, we will see an example of how to use a pre-trained Word2vec model for doing feature extraction and performing text classification.\n",
        "\n",
        "We will use the sentiment labelled sentences dataset from UCI repository\n",
        "http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "The dataset consists of 1500 positive, and 1500 negative sentiment sentences from Amazon, Yelp, IMDB. Let us first combine all the three separate data files into one using the following unix command:\n",
        "\n",
        "```cat amazon_cells_labelled.txt imdb_labelled.txt yelp_labelled.txt > sentiment_sentences.txt```\n",
        "\n",
        "For a pre-trained embedding model, we will use the Google News vectors.\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "\n",
        "Let us get started!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pkg_resources\n",
        "\n",
        "def get_library_versions(library_list):\n",
        "    frozen_list = []\n",
        "\n",
        "    for library in library_list:\n",
        "        try:\n",
        "            version = pkg_resources.get_distribution(library).version\n",
        "            frozen_list.append(f\"{library}=={version}\")\n",
        "        except pkg_resources.DistributionNotFound:\n",
        "            print(f\"Error: {library} not found or could not retrieve version.\")\n",
        "\n",
        "    return frozen_list\n",
        "\n",
        "# List of library names\n",
        "libraries = [\"numpy\", \"pandas\", \"gensim\", \"nltk\", \"scikit-learn\", \"gdown\"]\n",
        "\n",
        "# Get frozen list of library versions\n",
        "frozen_versions = get_library_versions(libraries)\n",
        "\n",
        "# Print the frozen list\n",
        "for item in frozen_versions:\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN4IKdaFCH7c",
        "outputId": "ad4e3a11-17ce-4049-8ce9-53972fe41bfb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy==1.23.5\n",
            "pandas==1.5.3\n",
            "gensim==4.3.1\n",
            "nltk==3.8.1\n",
            "scikit-learn==1.2.2\n",
            "gdown==4.6.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77UP8YyEdS2W",
        "outputId": "c7203e6a-e19d-4e9a-f577-ae936a2e1a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Requirement already satisfied: gensim==4.3.1 in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.1) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.1) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.1) (6.3.0)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (3.2.0)\n",
            "Requirement already satisfied: gdown==4.6.6 in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.6) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.6) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.6) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.6) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.6.6) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.6.6) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.6) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.6) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.6) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.6) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==4.6.6) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "!pip install numpy==1.23.5\n",
        "!pip install pandas==1.5.3\n",
        "!pip install gensim==4.3.1\n",
        "!pip install nltk==3.8.1\n",
        "!pip install scikit-learn==1.2.2\n",
        "!pip install gdown==4.6.6\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "URLGvBLv9T0M"
      },
      "outputs": [],
      "source": [
        "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "# try:\n",
        "#     import google.colab\n",
        "#     !curl  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/ch4-requirements.txt | xargs -n 1 -L 1 pip install\n",
        "# except ModuleNotFoundError:\n",
        "#     !pip install -r \"ch4-requirements.txt\"\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQX8DAmBb_Hr",
        "outputId": "65dc3618-5f7b-41a9-88bf-a5090b32f270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#basic imports\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "from time import time\n",
        "\n",
        "#pre-processing imports\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "#imports related to modeling\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#google-drive download imports\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S8RM8c6AS8AX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "\n",
        "    # upload 'amazon_cells_labelled.txt', 'imdb_labelled.txt' and 'yelp_labelled.txt' present in \"sentiment labelled sentences\" folder\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    !mkdir DATAPATH\n",
        "    !mv -t DATAPATH amazon_cells_labelled.txt imdb_labelled.txt yelp_labelled.txt\n",
        "    !cat DATAPATH/amazon_cells_labelled.txt DATAPATH/imdb_labelled.txt DATAPATH/yelp_labelled.txt > DATAPATH/sentiment_sentences.txt\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "\n",
        "    fil = 'sentiment_sentences.txt'\n",
        "\n",
        "    if not os.path.exists(\"Data/sentiment_sentences.txt\"):\n",
        "        file = open(os.path.join(path, fil), 'w')\n",
        "        file.close()\n",
        "\n",
        "        # combined the three files to make sentiment_sentences.txt\n",
        "        filenames = ['amazon_cells_labelled.txt', 'imdb_labelled.txt', 'yelp_labelled.txt']\n",
        "\n",
        "        with open('Data/sentiment_sentences.txt', 'w') as outfile:\n",
        "            for fname in filenames:\n",
        "                with open('Data/sentiment labelled sentences/' + fname) as infile:\n",
        "                    outfile.write(infile.read())\n",
        "        print(\"File created\")\n",
        "    else:\n",
        "        print(\"File already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COUGXAxcb_H5",
        "outputId": "640ac771-0389-4640-ab79-a11f26bd2c29",
        "scrolled": true
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
            "To: /content/DATAPATH/GoogleNews-vectors-negative300.bin.gz\n",
            "100% 1.65G/1.65G [00:15<00:00, 103MB/s]\n",
            "CPU times: user 28.1 s, sys: 3.92 s, total: 32.1 s\n",
            "Wall time: 34.4 s\n",
            "done loading Word2Vec\n"
          ]
        }
      ],
      "source": [
        "#Load the pre-trained word2vec model and the dataset\n",
        "\n",
        "def check_if_file_exists(filename: str, locations: list) -> str :\n",
        "    for location in locations:\n",
        "        if os.path.exists(os.path.join(location, filename)):\n",
        "            return location\n",
        "    return None\n",
        "\n",
        "def extract_data(location: str) -> None:\n",
        "    with gzip.open(os.path.join(location, 'GoogleNews-vectors-negative300.bin.gz'), 'rb') as f_in:\n",
        "        with open(os.path.join('./Data', './GoogleNews-vectors-negative300.bin'), 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    data_path= \"DATAPATH\"\n",
        "    !gdown -O DATAPATH/ https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
        "    !gunzip DATAPATH/GoogleNews-vectors-negative300.bin.gz\n",
        "    path_to_model = 'DATAPATH/GoogleNews-vectors-negative300.bin'\n",
        "    training_data_path = \"DATAPATH/sentiment_sentences.txt\"\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "\n",
        "    data_path = './Data/'\n",
        "    compressed_file_name = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "    extracted_file_name = 'GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "    # Check if Extracted File exists\n",
        "    location_of_extracted_file = check_if_file_exists(extracted_file_name, ['./Data','../Ch2/Data','../Ch3/Data'])\n",
        "\n",
        "    if location_of_extracted_file:\n",
        "        # Extracted File exists\n",
        "        path_to_model = os.path.join(location_of_extracted_file, extracted_file_name)\n",
        "\n",
        "    else:\n",
        "        location_of_compressed_file = check_if_file_exists(compressed_file_name, ['./Data','../Ch2/Data','../Ch3/Data'])\n",
        "\n",
        "        if location_of_compressed_file:\n",
        "            # Compressed File exists\n",
        "            extract_data(os.path.join(location_of_compressed_file))\n",
        "            path_to_model = os.path.join(data_path, extracted_file_name)\n",
        "\n",
        "        else:\n",
        "            # Download File\n",
        "            output_path = './Data/'\n",
        "            gdown.download(\"https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\", output=output_path)\n",
        "\n",
        "            # Extract File\n",
        "            extract_data(output_path)\n",
        "\n",
        "            path_to_model = os.path.join(data_path, extracted_file_name)\n",
        "\n",
        "    print(f\"Data Present at location : {path_to_model}\")\n",
        "    training_data_path = os.path.join(data_path, \"sentiment_sentences.txt\")\n",
        "\n",
        "\n",
        "#Load W2V model. This will take some time.\n",
        "%time w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
        "print('done loading Word2Vec')\n",
        "\n",
        "#Read text data, cats.\n",
        "#the file path consists of tab separated sentences and cats.\n",
        "texts = []\n",
        "cats = []\n",
        "fh = open(training_data_path)\n",
        "for line in fh:\n",
        "    text, sentiment = line.split(\"\\t\")\n",
        "    texts.append(text)\n",
        "    cats.append(sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-WjFyC6b_IE",
        "outputId": "7cb1a092-d3fa-4bf4-e437-6d079da7ed74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000000\n"
          ]
        }
      ],
      "source": [
        "#Inspect the model\n",
        "word2vec_vocab = w2v_model.key_to_index.keys()\n",
        "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
        "print(len(word2vec_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEz30Jztb_IP",
        "outputId": "7c37e0e1-9f2e-411b-cdac-b89ecc39a0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000 3000\n",
            "Good case, Excellent value.\n",
            "1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Inspect the dataset\n",
        "print(len(cats), len(texts))\n",
        "print(texts[1])\n",
        "print(cats[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFOGaDTwb_Ig",
        "outputId": "4e50a4a9-1f40-429c-c7b3-e445e42cae6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000 3000\n",
            "['good', 'case', 'excellent', 'value']\n",
            "1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#preprocess the text.\n",
        "def preprocess_corpus(texts):\n",
        "    mystopwords = set(stopwords.words(\"english\"))\n",
        "    def remove_stops_digits(tokens):\n",
        "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
        "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit()\n",
        "               and token not in punctuation]\n",
        "    #This return statement below uses the above function to process twitter tokenizer output further.\n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
        "\n",
        "texts_processed = preprocess_corpus(texts)\n",
        "print(len(cats), len(texts_processed))\n",
        "print(texts_processed[1])\n",
        "print(cats[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXRiGtY1b_Iq",
        "outputId": "1f5eaad6-939e-46fc-cf27-21798f78e18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n"
          ]
        }
      ],
      "source": [
        "# Creating a feature vector by averaging all embeddings for all sentences\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 300\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists:\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0 + 1e-5 # to avoid divide-by-zero\n",
        "        for token in tokens:\n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this +=1\n",
        "        if(count_for_this!=0):\n",
        "            feats.append(feat_for_this/count_for_this)\n",
        "        else:\n",
        "            feats.append(zero_vector)\n",
        "    return feats\n",
        "\n",
        "\n",
        "train_vectors = embedding_feats(texts_processed)\n",
        "print(len(train_vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr9IaQppb_Ix",
        "outputId": "c74d84ea-6586-4d68-c8f7-e2e36b7f915d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8013333333333333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0\n",
            "       0.77      0.83      0.80       353\n",
            "          1\n",
            "       0.84      0.78      0.81       397\n",
            "\n",
            "    accuracy                           0.80       750\n",
            "   macro avg       0.80      0.80      0.80       750\n",
            "weighted avg       0.80      0.80      0.80       750\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Take any classifier (LogisticRegression here, and train/test it like before.\n",
        "classifier = LogisticRegression(random_state=1234)\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\n",
        "classifier.fit(train_data, train_cats)\n",
        "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
        "preds = classifier.predict(test_data)\n",
        "print(classification_report(test_cats, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7wjLB8rb_JB"
      },
      "source": [
        "Not bad. With little efforts we got 80% accuracy. Thats a great starting model to have!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "h4lF7mkPCAuy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}